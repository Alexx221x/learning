{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning as a mixed convex-combinatorial optimization problem\n",
    "https://arxiv.org/abs/1710.11573\n",
    "* Hard activations, can't use gradient descent\n",
    "* Network quantization -> reduce time and energy requirements\n",
    "* Create large integrated systems which may have non-differentiable components and must avoid vanishing/exploding gradients\n",
    "* Settings targets for hard-threshold hidden units in order to minimize loss is a discrete optimization problem\n",
    "* Each unit has a linearly separable problem to solve -> network decomposes into individual perceptrons\n",
    "* [Straight Through Estimation (STE)](https://arxiv.org/abs/1308.3432) is a special case\n",
    "* Feasible target propagation (FRPROP), recursive\n",
    "* STE can lead to gradient mismatch error???\n",
    "* A perceptron can be learned for a non-linearly-separable dataset by minimizing the hinge loss, a convex loss on the perceptron's pre-activation output z, and target t that maximizes the margin when combined with L2 regularization\n",
    "\n",
    "### FTPROP\n",
    "* Convolutional layer imposes structure on weight matrix -> less likely to be linearly separable. Feasbility constraint relaxed.\n",
    "* FTPROP-MB (minibatch) closesly resembles backpropagation-based methods\n",
    "* Derivatives cannot be propagated through layers but can be computed within a layer\n",
    "* Hinge loss -> learning stalled and erratic due to (1) sensitivity to noise (2) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
