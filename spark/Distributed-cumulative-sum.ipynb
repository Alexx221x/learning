{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Cumulative Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting `spark` up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classpath.add(\n",
    "  \"org.apache.spark\" %% \"spark-core\" % \"2.0.2\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.{SparkSession, DataFrame, Dataset}\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{SparkSession, DataFrame, Dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val spark = SparkSession.builder().master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[0m: \u001b[32mSeq\u001b[0m[\u001b[32mInt\u001b[0m] = \u001b[33mList\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m4\u001b[0m, \u001b[32m5\u001b[0m)\n",
       "\u001b[36mexpected\u001b[0m: \u001b[32mSeq\u001b[0m[\u001b[32mInt\u001b[0m] = \u001b[33mList\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m6\u001b[0m, \u001b[32m10\u001b[0m, \u001b[32m15\u001b[0m)\n",
       "\u001b[36mlocal\u001b[0m: \u001b[32mSeq\u001b[0m[\u001b[32mInt\u001b[0m] = \u001b[33mList\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m6\u001b[0m, \u001b[32m10\u001b[0m, \u001b[32m15\u001b[0m)\n",
       "\u001b[36mrdd\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mInt\u001b[0m] = ParallelCollectionRDD[39] at parallelize at Main.scala:36"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Let's assume we have some sorted data that we want to calculate the cumulative sum for\n",
    "val data = Seq(1, 2, 3, 4, 5)\n",
    "\n",
    "// Here's the expected cumulative sum\n",
    "val expected = Seq(1, 3, 6, 10, 15)\n",
    "\n",
    "// If this was a local Iterator we could just use scanLeft\n",
    "val local = data.scanLeft(0)(_ + _).drop(1)\n",
    "\n",
    "// But what if it's distributed?\n",
    "val rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mx\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mInt\u001b[0m, \u001b[32mInt\u001b[0m] = \u001b[33mMap\u001b[0m(\u001b[32m0\u001b[0m -> \u001b[32m0\u001b[0m, \u001b[32m5\u001b[0m -> \u001b[32m0\u001b[0m, \u001b[32m1\u001b[0m -> \u001b[32m1\u001b[0m, \u001b[32m6\u001b[0m -> \u001b[32m4\u001b[0m, \u001b[32m2\u001b[0m -> \u001b[32m0\u001b[0m, \u001b[32m7\u001b[0m -> \u001b[32m5\u001b[0m, \u001b[32m3\u001b[0m -> \u001b[32m2\u001b[0m, \u001b[32m4\u001b[0m -> \u001b[32m3\u001b[0m)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Calculate the sum per partition\n",
    "val x = rdd.mapPartitionsWithIndex { (index, partition) =>\n",
    "    Iterator((index, partition.sum))\n",
    "}.collect().toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres47\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mInt\u001b[0m] = \u001b[33mArray\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m6\u001b[0m, \u001b[32m10\u001b[0m, \u001b[32m15\u001b[0m)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd.mapPartitionsWithIndex { (index, partition) =>\n",
    "    // For each partition calculate the sum of all the previous partitions\n",
    "    val sums = (0 until index).map(x).sum\n",
    "    \n",
    "    // Scan left starting with the cumulative sum for all previous partitions\n",
    "    partition.scanLeft(sums)(_ + _).drop(1)\n",
    "}.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mx\u001b[0m: \u001b[32mMap\u001b[0m[\u001b[32mInt\u001b[0m, \u001b[32mInt\u001b[0m] = \u001b[33mMap\u001b[0m(\u001b[32m0\u001b[0m -> \u001b[32m0\u001b[0m, \u001b[32m5\u001b[0m -> \u001b[32m6\u001b[0m, \u001b[32m1\u001b[0m -> \u001b[32m1\u001b[0m, \u001b[32m6\u001b[0m -> \u001b[32m10\u001b[0m, \u001b[32m2\u001b[0m -> \u001b[32m1\u001b[0m, \u001b[32m7\u001b[0m -> \u001b[32m15\u001b[0m, \u001b[32m3\u001b[0m -> \u001b[32m3\u001b[0m, \u001b[32m4\u001b[0m -> \u001b[32m6\u001b[0m)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Calculate the cumulative sum at each partition index once\n",
    "val x = rdd.mapPartitionsWithIndex { (index, partition) =>\n",
    "    Iterator((index, partition.sum))\n",
    "}.collect().scanLeft((0, 0))((a, b) => (b._1, a._2 + b._2)).toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres45\u001b[0m: \u001b[32mArray\u001b[0m[\u001b[32mInt\u001b[0m] = \u001b[33mArray\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m6\u001b[0m, \u001b[32m10\u001b[0m, \u001b[32m15\u001b[0m)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd.mapPartitionsWithIndex { (index, partition) =>\n",
    "    partition.scanLeft(x.getOrElse(index - 1, 0) )(_ + _).drop(1)\n",
    "}.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to [jmorra](https://github.com/jmorra) for teaching me this."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
