{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-policy evaluation for slate recommendation\n",
    "https://arxiv.org/pdf/1605.04812.pdf\n",
    "- [Combinatorial bandits](https://arxiv.org/abs/1502.03475)\n",
    "- Uses log data to estimate policy performance\n",
    "- Policy -> content-serving algorithm\n",
    "- Inverse Propensity Scores (IPS) require lots of data\n",
    "- Challenge -> # of possible lists (i.e. slates) is combinatorially large so the policy is likely to choose slates that are different from the data collected\n",
    "- Previous work\n",
    "    - (1) restricts the attention to a small slate space to increase the chance of matching\n",
    "    - (2) used parametric models.\n",
    "    - They need less data but have large bias and need long A/B testing\n",
    "    \n",
    "### Combinatorial contextual bandits formulation\n",
    "- For each context -> policy selects a slate consisting of component actions\n",
    "- Reward is observed for the entire slate (e.g. time to success, NDCG)\n",
    "- \n",
    "\n",
    "### On vs. off policy\n",
    "[StackExchange](https://stats.stackexchange.com/questions/184657/difference-between-off-policy-and-on-policy-learning):\n",
    "```\n",
    "The difference between Off-policy and On-policy methods is that with the first you do not need to follow any specific policy, your agent could even behave randomly and despite this, off-policy methods can still find the optimal policy. On the other hand on-policy methods are dependent on the policy used. In the case of Q-Learning, which is off-policy, it will find the optimal policy independent of the policy used during exploration, however this is true only when you visit the different states enough times.\n",
    "```\n",
    "\n",
    "Similar answer from the same post:\n",
    "```\n",
    "The reason that Q-learning is off-policy is that it updates its Q-values using the Q-value of the next state s′ and the **greedy action a′**. In other words, it estimates the return (total discounted future reward) for state-action pairs assuming a greedy policy were followed despite the fact that it's not following a greedy policy.\n",
    "\n",
    "The reason that SARSA is on-policy is that it updates its Q-values using the Q-value of the next state s′s′ and the **current policy's action a″**. It estimates the return for state-action pairs assuming the current policy continues to be followed.\n",
    "```\n",
    "\n",
    "[More reading](https://courses.engr.illinois.edu/cs440/fa2009/lectures/Lect24.pdf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
