{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandit based Monte-Carlo planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Upper Confidence Tree (UCT)\n",
    "* UCT applies bandit ideas to guide Monte-Carlo planning\n",
    "* Used in finding near-optimal action in large state-space MDP when we have a generative model of MDP\n",
    "* The main idea is to **sample actions selectively** with **UCB1** and call it Upper Confidence applied to Trees (UCT)\n",
    "    * If we can restrict sampling to 1/2 of the actions we're reducing work by (1/2)^D where D is the lookahead depth\n",
    "    * Action is considered sub-optimal (by definition) if its value is less than the best action\n",
    "    * Action values depend on the value of successor states\n",
    "    * We need to get the estimation error of the values to decay fast\n",
    "        * This leads to the typical exploration-exploitation dilemma\n",
    "* The sampling probability of actions at children node is changing and payoff sequences drift. UCT replaces the bias term with a term that takes this into account.\n",
    "* Action selection is framed as a multi-armed bandit for each internal node of the tree. \n",
    "\n",
    "**Kearns et al sampling based lookahead search:**\n",
    "* Root is the initial state where we start the planning\n",
    "* Tree with node labels alternating between states and state-actions\n",
    "* Nodes labeled by **state** followed by a fixed # of nodes associated with actions at that state\n",
    "* Nodes labeled by **state-action** followed by fixed # of state-labelled nodes *sampling* from the generative model of the MDP\n",
    "* Sampled rewards stored with the edge connecting state-action and state nodes\n",
    "* State-action node value computed based on average of the sum of the rewards along the edges originating at the node and the successor node values\n",
    "* State node value is the max of the values of its children\n",
    "* Fixed sized trees are sufficient under certain conditions\n",
    "    * Depth is a function of ε and γ\n",
    "    * Width is a function of K (number of actions), ε and γ\n",
    "* Promising but expensive\n",
    "* This work achieves 2 characteristics over this vanilla MC method:\n",
    "    1. Small error probability if stopped prematurely\n",
    "    2. Convergence to the best action eventually\n",
    "    \n",
    "**Rollout-based planning**\n",
    "* The Kerans method is stage-wise tree building\n",
    "* Rollout-based builds the lookahead tree by **repeatedly sampling episodes from the initial state**\n",
    "* If we keep encountering the same states then we can bias the choice of the actions to follow and potentially convergence faster\n",
    "* If we don't see the same states this method degenerates to the non-selective vanilla method\n",
    "\n",
    "**Algorithm scheme**\n",
    "* Generate episodes and return highest long-term reward action\n",
    "* Update value at a given depth and increase counter for state-action\n",
    "\n",
    "**UCB1**\n",
    "* Regret is loss from not always picking the best arm\n",
    "* For a large class of payoff distributions there is no policy with regret growing slower than O(ln n). For these the exploration-exploitation dilemma is resolved if **regret growth rate** is within a **constant factor** of the best possible regret rate\n",
    "* Keeps track of average reward per arm and chooses the arm with the best upper confidence bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- http://ggp.stanford.edu/readings/uct.pdf\n",
    "- https://github.com/bonetblai/mdp-engine"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
