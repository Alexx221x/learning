{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nodes: decision nodes\n",
    "* Edges: values\n",
    "* Leafs: output / decision / answer\n",
    "* Inductive inference\n",
    "* Robust to noisy data\n",
    "* Can learn disjunctive expressions\n",
    "* Completely expressive hypothesis space\n",
    "    * Capable of expressing any finite discrete-valued function\n",
    "    * Searches incompletely through this space\n",
    "* Represent a disjunction of conjunctions\n",
    "    * Each path to a leaf is a conjunction\n",
    "    * The tree as a whole is a disjunction\n",
    "* Handle missing values\n",
    "* Variants: ID3, ASSISTANT, C4.5\n",
    "* Greedy search (never backtracks)\n",
    "\n",
    "### Inductive bias\n",
    "- In general some sort of inductive bias is necessary to generalize, otherwise we're overfitting\n",
    "    - Preference bias > restriction bias\n",
    "- Restriction/language bias\n",
    "    - Completely expressive\n",
    "- Preference/search bias\n",
    "    - Prefers good splits early on\n",
    "    - Prefers correct trees\n",
    "    - Prefers shorter trees (a consequence of the 1st preference)\n",
    "        - Occam's razor\n",
    "        \n",
    "### Occam's razor\n",
    "- William of Occam, 1320\n",
    "- Why shorter hypotheses are better?\n",
    "    - Combinatorial arguments: more complex hypothesis exist than simple ones so the probability of finding a shorter hypothesis that also fits the data is also smaller. In other words there's less of a chance of a spurious fit.\n",
    "        - Different learners with different internal representations might arrive at contradictory hypothesis, both try to justify with Occam's razor\n",
    "        \n",
    "### Entropy\n",
    "* How homogenous/pure are the examples in a set\n",
    "* If perfectly pure then entropy is 0 (if all is the same value)\n",
    "* Entropy is maximum for a given cardinality (i.e. number of unique items in a set) when no item is repeated\n",
    "\n",
    "$$-\\sum_{v}p(v)logp(v)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(x):\n",
    "    _, counts = np.unique(x, return_counts=True)\n",
    "    p = counts / np.sum(counts)\n",
    "    return -np.sum(p * np.log2(p))\n",
    "\n",
    "def entropy_binary(x):\n",
    "    trues = np.sum(x)\n",
    "    percentage_true = trues / len(x)\n",
    "    if percentage_true == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        percentage_false = 1 - percentage_true\n",
    "        p = np.array([percentage_true, percentage_false])\n",
    "        return -np.sum(p * np.log2(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [True, False, False, False]\n",
    "x2 = [True, False]\n",
    "x3 = [True, True, True]\n",
    "x4 = [True] * 99 + [False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81127812445913283"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.080793135895911181"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81127812445913283"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_binary(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_binary(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_binary(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.080793135895911236"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: not exactly the same\n",
    "entropy_binary(x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information gain\n",
    "$$Gain(S, A)=Entropy(S)-\\sum_{v}\\frac{|S_v|}{|S|}Entropy(S_v)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    ({'attribute_values': [True, True, False, False], 'ys': [True, True, False, False]}, 1.0),\n",
    "    ({'attribute_values': [True, True, False, False], 'ys': [False, True, False, True]}, 0.0),\n",
    "    ({'attribute_values': [False, True, False, False], 'ys': [False, True, False, True]}, 0.31127812445913283),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain_naive(attribute_values, ys):\n",
    "    e = entropy(ys)\n",
    "    \n",
    "    a_stats = {True: [], False: []}\n",
    "    for a, y in zip(attribute_values, ys):\n",
    "        a_stats[a].append(y)\n",
    "    \n",
    "    a_entropy = sum([\n",
    "        len(a_stats[val]) * entropy(a_stats[val])\n",
    "        for val in a_stats\n",
    "    ]) / len(ys)\n",
    "    return e - a_entropy\n",
    "\n",
    "def information_gain_vectorized(attribute_values, ys):\n",
    "    attribute_values = np.array(attribute_values)\n",
    "    ys = np.array(ys)\n",
    "    e = entropy(ys)\n",
    "    true_mask = attribute_values == True\n",
    "    ys_true = ys[true_mask]\n",
    "    ys_false = ys[~true_mask]\n",
    "    a_entropy = (len(ys_true) * entropy(ys_true)\n",
    "                 + len(ys_false) * entropy(ys_false)) / len(ys)\n",
    "    return e - a_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([\n",
    "    information_gain_naive(**inputs) == expected_output\n",
    "    for inputs, expected_output\n",
    "    in test_cases\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([\n",
    "    information_gain_vectorized(**inputs) == expected_output\n",
    "    for inputs, expected_output\n",
    "    in test_cases\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "av = [True] * 10000 + [False] * 20000 + [True] * 30000\n",
    "ys = [False] * 10000 + [True] * 20000 + [True] * 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 7.08 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "information_gain_vectorized(av, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 25.9 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "information_gain_naive(av, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3: top-down algorithm for inducing a decision tree\n",
    "* No backtracking\n",
    "* May get stuck in local optimum similar to hill-climbing search\n",
    "* Uses all the training examples at each step\n",
    "    * In constrast with incremental method (e.g. Find-S or Candidate-Elimination)\n",
    "    * Advantage: less sensitive to errors in individual training example (i.e. noise)\n",
    "* Efficient approximation to BFS-ID3\n",
    "    * BFS-ID3 is a breadth-first search over trees of increasing depth\n",
    "* Uses information gain heuristic + hill climbing\n",
    "* \n",
    "\n",
    "- Loop:\n",
    "    - Pick the best attribute A to split on\n",
    "    - Assign A as decision node\n",
    "    - Assign examples to leafs for each possible value of A\n",
    "    - Stop if the stopping criterion is met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    ({'xs': [{}, {}], 'ys': [True, True]}, {'root': True}),  # no attributes\n",
    "    ({'xs': [{}, {}, {}], 'ys': [True, False, False]}, {'root': False}),\n",
    "    ({'xs': [{'a': True}, {'a': False}], 'ys': [True, True]} , {'root': True}),  # all labels the same\n",
    "    ({'xs': [{'a': True}, {'a': False}], 'ys': [False, False]} , {'root': False}),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3:\n",
    "    def __init__(self):\n",
    "        self.nodes = {}\n",
    "        self.current_key = 'root'\n",
    "        self.attributes = None\n",
    "        \n",
    "    def _best_attribute(self, xs, ys):\n",
    "        if self.attributes is None:\n",
    "            # Assuming all rows have the same attributes (dense features)\n",
    "            self.attributes = set(xs[0].keys())\n",
    "        \n",
    "        return max([\n",
    "            (information_gain_vectorized([\n",
    "                instance[attribute]\n",
    "                for instance in xs\n",
    "            ], ys), attribute)\n",
    "            for attribute in self.attributes\n",
    "        ], key=lambda x: x[0])[1]\n",
    "        \n",
    "    \n",
    "    def train(self, xs, ys):\n",
    "        unique_ys, unique_y_counts = np.unique(ys, return_counts=True)\n",
    "        if all([len(x) == 0 for x in xs]):\n",
    "            # No attributes\n",
    "            self.nodes[self.current_key] = unique_ys[np.argmax(unique_y_counts)]\n",
    "        else:\n",
    "            if len(unique_ys) == 1:\n",
    "                # All labels the same\n",
    "                self.nodes[self.current_key] = unique_ys[0]\n",
    "            else:\n",
    "                best_attribute = self._best_attribute(xs, ys)\n",
    "                self.current_key = best_attribute\n",
    "                x_left = None\n",
    "                x_right = None\n",
    "                y_left = None\n",
    "                y_right = None\n",
    "                self[key] = self.train(x_left)\n",
    "                \n",
    "                \n",
    "        return self.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID3().train([{'a': True, 'b': False},\n",
    "             {'a': True, 'b': False},\n",
    "             {'a': False, 'b': False},\n",
    "             {'a': False, 'b': True}], [True, True, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([\n",
    "    ID3().train(**inputs) == expected_output\n",
    "    for inputs, expected_output\n",
    "    in test_cases\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- https://www.udacity.com/course/machine-learning--ud262\n",
    "- [Machine Learning by Tom Mitchell](https://www.amazon.com/Learning-McGraw-Hill-International-Editions-Computer/dp/0071154671/ref=asap_bc?ie=UTF8)\n",
    "- https://blog.slavv.com/identifying-churn-drivers-with-random-forests-65bad0193e6b"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
